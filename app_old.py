"""This is the back-end app for the project Emotion Recognition in Multimedia Content.
Main requirements:
- receives a file and identify which modalities are in the file (text, audio, video etc...)
- acording to the files it uses a model to identify emotion in the existent modalities (one model for each modality)
- an LLM powered by ollama to generate a report with the results of the analysis
- service that works with the chat interaction with ollama model
"""

from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
import os
import mimetypes
import requests
from src.utils import load_model_with_weights
from ollama import Client as Ollama
from sklearn.svm import SVC
import librosa

from flask_restx import Api, Namespace, fields, Resource

# Initialize Flask application
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = './uploads'
app.config['ALLOWED_EXTENSIONS'] = {'txt', 'wav', 'mp3', 'mp4', 'avi'}
app.config['OLLAMA_URL'] = 'http://localhost:11434/api'

# Initialize Flask-RESTx Api
api = Api(app,
          version='1.0',
          title='Emotion API',
          description='Multimodal Emotion Analysis API',
          doc='/docs')

# Define namespaces
analysis_ns = Namespace('Analysis', description='File analysis operations')
chat_ns = Namespace('Chat', description='Chat interaction with the LLM')

# Add namespaces to the API
api.add_namespace(analysis_ns)
api.add_namespace(chat_ns)

# Model for Swagger documentation for Analysis
analysis_response_model = analysis_ns.model('AnalysisResult', {
    'modality': fields.String(description='Detected modality of the file'),
    'analysis': fields.Raw(description='Analysis results for the modality'),
    'report': fields.String(description='Report generated by the LLM')
})

# Model for Swagger documentation for Chat
chat_request_model = chat_ns.model('ChatRequest', {
    'message': fields.String(required=True, description='User message'),
    'history': fields.List(fields.Raw, description='Chat history (list of {"role": "user/assistant", "content": "..."})')
})

chat_response_model = chat_ns.model('ChatResponse', {
    'response': fields.Raw(description='LLM response')
})

# Mock models (replace with actual implementations)
class EmotionModels:
    @staticmethod
    def analyze_text(text):
        return {"emotions": [{"text": "happy", "confidence": 0.85}]}

    @staticmethod
    def analyze_audio(audio_path):
        return {"emotions": [{"audio": "neutral", "confidence": 0.78}]}

    @staticmethod
    def analyze_video(video_path):
        return {"emotions": [{"video": "surprise", "confidence": 0.92}]}

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def detect_modality(file_path):
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type:
        if mime_type.startswith('video'):
            return 'video'
        elif mime_type.startswith('audio'):
            return 'audio'
        elif mime_type.startswith('text'):
            return 'text'
    return 'unknown'

def analyze_file(file_path, modality):
    """Analyze the file based on its modality."""
    # load models
    text_model = Ollama(model="tinyllama", url=app.config['OLLAMA_URL'])
    audio_model = load_model_with_weights('emotion_model.h5') # Placeholder
    facial_model = load_model_with_weights('fer2013_model_vgg.h5') # Placeholder
    analysis_results = {}

    if modality == 'text':
        with open(file_path, 'r') as f:
            text_content = f.read()
        # ask to ollama llm what does it predict in each sentence
        analysis_results['text'] = text_model.chat(messages=[
            {'role': 'user', 'content': f"Analyze the following text spoken by a person, please identify the emotion underlying each sentence:\n{text_content}"}
        ])

    elif modality == 'audio':
        try:
            # split audio in parts of 5s and analise each piece of audio
            y, sr = librosa.load(file_path, sr=16000)
            audio_chunks = librosa.effects.split(y, top_db=20)
            audio_analysis = []
            for start, end in audio_chunks:
                chunk = y[start:end]
                # save chunk to a temporary file
                chunk_path = os.path.join(app.config['UPLOAD_FOLDER'], 'chunk.wav')
                librosa.output.write_wav(chunk_path, chunk, sr=sr)
                # analyze each chunk (replace with actual audio analysis)
                audio_analysis.append(EmotionModels.analyze_audio(chunk_path))
                os.remove(chunk_path)
            analysis_results['audio'] = audio_analysis
        except Exception as e:
            analysis_results['audio'] = {"error": f"Audio analysis failed: {e}"}

    elif modality == 'video':
        analysis_results['video'] = EmotionModels.analyze_video(file_path) # Placeholder

    return analysis_results

def generate_report(analysis_data):
    prompt = f"""
    Generate a comprehensive emotion analysis report based on the following data:
    {analysis_data}

    Include:
    1. Summary of dominant emotions across modalities
    2. Notable patterns or contradictions
    3. Behavioral insights
    4. Recommendations for emotional awareness
    """

    try:
        response = requests.post(
            f"{app.config['OLLAMA_URL']}/generate",
            json={
                "model": "tinyllama",
                "prompt": prompt,
                "stream": False
            }
        )
        response.raise_for_status()  # Raise an exception for HTTP errors
        return response.json().get('response', 'Report generation failed')
    except requests.exceptions.RequestException as e:
        return f"Report generation failed: {e}"

@analysis_ns.route('/analyze')
@analysis_ns.doc(description='Analyze multimedia content for emotional data')
class AnalysisResource(Resource):
    @analysis_ns.expect({'file': 'binary'}, validate=True)
    @analysis_ns.marshal_with(analysis_response_model, code=200)
    @analysis_ns.response(400, 'Invalid file')
    def post(self):
        """Analyze multimedia content."""
        if 'file' not in request.files:
            analysis_ns.abort(400, 'No file uploaded')

        file = request.files['file']
        if file.filename == '':
            analysis_ns.abort(400, 'Empty filename')

        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(file_path)

            modality = detect_modality(file_path)
            if modality == 'unknown':
                os.remove(file_path)
                analysis_ns.abort(400, 'Unsupported file type')

            analysis_results = analyze_file(file_path, modality)
            report = generate_report(analysis_results)
            os.remove(file_path)

            return {
                "modality": modality,
                "analysis": analysis_results,
                "report": report
            }

        analysis_ns.abort(400, 'Invalid file')

@chat_ns.route('/chat')
@chat_ns.doc(description='Interact with the LLM for chat')
class ChatResource(Resource):
    @chat_ns.expect(chat_request_model, validate=True)
    @chat_ns.marshal_with(chat_response_model, code=200)
    def post(self):
        """Chat with the LLM."""
        data = request.json
        message = data.get('message')
        history = data.get('history', [])

        try:
            response = requests.post(
                f"{app.config['OLLAMA_URL']}/chat",
                json={
                    "model": "llama2",
                    "messages": history + [{"role": "user", "content": message}],
                    "stream": False
                }
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": f"Chat with LLM failed: {e}"}

if __name__ == '__main__':
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    app.run(host='0.0.0.0', port=5000, debug=True)